{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言\n",
    "本项目对文档进行分类，采用擅长文本分类的朴素贝叶斯算法，通过计算文档中每个单词的 TF-IDF 值，取 TF-IDF 值最高的单词作为区分，从而达到分类文档的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba  # 中文分词工具\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # 计算单词 TF-IDF 值\n",
    "from sklearn.naive_bayes import MultinomialNB  # 多项式朴素贝叶斯\n",
    "from sklearn import metrics  # 评估函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'\\xef\\xbb\\xbf,',\n",
       " b'?',\n",
       " b'\\xe3\\x80\\x81',\n",
       " b'\\xe3\\x80\\x82',\n",
       " b'\\xe2\\x80\\x9c',\n",
       " b'\\xe2\\x80\\x9d',\n",
       " b'\\xe3\\x80\\x8a',\n",
       " b'\\xe3\\x80\\x8b',\n",
       " b'\\xef\\xbc\\x81',\n",
       " b'\\xef\\xbc\\x8c']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载停用词，避免编码失败采用二进制方式读取\n",
    "with open(r\"text classification\\stop\\stopword.txt\",'rb') as f:\n",
    "    stop_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "stop_words[:10]  # 查看读取结果的前10个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 定义函数，加载数据\n",
    "def load_data(base_path):\n",
    "    '''\n",
    "    base_path: 基础路径\n",
    "    '''\n",
    "\n",
    "    documents = []  # 分词列表\n",
    "    labels = []     # 标签列表\n",
    "\n",
    "    # 循环所有文件并进行分词打标\n",
    "    for path, folder, files in os.walk(base_path):  # 路径、文件夹、文件\n",
    "        for file in files:\n",
    "            label = path.split('\\\\')[-1]  # 转义\n",
    "            labels.append(label)\n",
    "            filename = os.path.join(path, file)   # 把路径和文件名合成一个完整路径\n",
    "            with open(filename, 'rb') as f:  # 用二进制方式读取\n",
    "                content = f.read()\n",
    "                words = list(jieba.cut(content))\n",
    "                documents.append(' '.join(words))\n",
    "\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造模型，评估准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数，构造模型并计算测试集准确率\n",
    "def train_fun(train_data, train_label, test_data, test_label):\n",
    "    '''\n",
    "    train_data: 训练集数据\n",
    "    train_label: 训练集标签\n",
    "    test_data: 测试集数据\n",
    "    test_label: 测试集标签\n",
    "    '''\n",
    "\n",
    "    # 创建训练集 TfidfVectorizer 类，max_df=0.5 代表一个单词在 50% 的文档中都出现过,不作为分词统计\n",
    "    train_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\n",
    "\n",
    "    # 训练集拟合，得到训练集 TF-IDF 特征空间 features\n",
    "    train_features = train_tf.fit_transform(train_data)\n",
    "\n",
    "    # 构造多项式朴素贝叶斯分类器，alpha 越小，迭代次数越多，精度越高\n",
    "    clf = MultinomialNB(alpha=0.001).fit(train_features, train_label)\n",
    "\n",
    "    # 创建测试集 TfidfVectorizer 类，词汇表来自训练集tf\n",
    "    test_tf = TfidfVectorizer(stop_words=stop_words,\n",
    "                              max_df=0.5,\n",
    "                              vocabulary=train_tf.vocabulary_)\n",
    "    # 测试集拟合，得到测试集 TF-IDF 特征空间 test_features\n",
    "    test_features = test_tf.fit_transform(test_data)\n",
    "\n",
    "    # 测试集预测\n",
    "    predicted_labels = clf.predict(test_features)\n",
    "\n",
    "    # 评估准确率\n",
    "    score = metrics.accuracy_score(test_label, predicted_labels)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据，评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\鹏鹏\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.620 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n"
     ]
    }
   ],
   "source": [
    "# 加载训练集数据\n",
    "train_documents, train_labels = load_data(r\"text classification\\train\")\n",
    "\n",
    "# 加载测试集数据\n",
    "test_documents, test_labels = load_data(r\"text classification\\test\")\n",
    "\n",
    "# 评估模型\n",
    "score = train_fun(train_documents, train_labels, test_documents, test_labels)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明文档分类有91%的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目是用贝叶斯分类器对中文文档进行分类，将文档分成四大类：女性、体育、文学、校园。采用中文分词工具 jieba ，这个工具可以对中文文档提取分词，基于这些分词，我们得到分词的权重，即特征矩阵。通过特征矩阵与分类结果，我们创建出朴素贝叶斯分类器，然后用分类器进行预测，最后将预测结果与实际结果做对比，可以得到分类器在测试集上的准确率。\n",
    "\n",
    "文档分类的原理是通过计算文档中单词的 TF-IDF 值，TF-IDF 值高，意味着这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。因此这样的单词适合用于分类。另外，在中文文档里，“你，我，并且，一切，所以”等等这些词在分类中没有用，起不到分类作用，为了节省内存和计算时间，我们不计算这些词的 TF-IDF 值，把这些词作为停用词 stop words， TfidfVectorizer 工具就会自动忽略掉这些词，不会计算这些词的 TF-IDF 值。\n",
    "\n",
    "通过模型预测，可以发现，在这个项目里，模型的预测准确率有 91%，说明模型预测准确率比较高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
